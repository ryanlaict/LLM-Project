{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing and installing"
      ],
      "metadata": {
        "id": "-_fVjZgPhSFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT28DkQhg81J"
      },
      "outputs": [],
      "source": [
        "#Pip install required\n",
        "\n",
        "!pip install datasets\n",
        "!pip install transformers datasets evaluate\n",
        "!pip install --upgrade torch torchvision torchaudio\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install --upgrade torch torchvision torchaudio\n",
        "!pip install --upgrade evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import os\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import spacy\n",
        "import csv\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "NDpTIE2IhC6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing and Cleaning"
      ],
      "metadata": {
        "id": "0euSFg-AhVmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "\n",
        "# Convert dataset to Pandas\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "display(df.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ORPPfH2BhElD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = df_sample['label'].value_counts()\n",
        "print(label_counts)\n",
        "\n",
        "null_values = df_sample.isnull().sum()\n",
        "print(null_values)"
      ],
      "metadata": {
        "id": "eWsgPiuXiN0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create function to generally clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # lowercase words\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove links\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"\", text)  # Remove mentions\n",
        "    text = re.sub(r\"#[A-Za-z0-9_]+\", \"\", text)  # Remove hashtags\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  #Uniform spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "#Create function to remove stop words\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#Lemmatize words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "#Apply cleaning\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "df['clean_text'] = df['clean_text'].apply(remove_stopwords)\n",
        "df['clean_text'] = df['clean_text'].apply(lemmatize_text)\n",
        "print(\"\\Cleaned Words\")\n",
        "\n",
        "#Print head to confirm cleaning\n",
        "display(df[['text', 'clean_text']].head())\n",
        "\n",
        "#Create subset to speed up data\n",
        "df_sample = df.sample(10000, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "r2qnCiKfh6EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create subset to speed up data\n",
        "df_sample = df.sample(10000, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "uJIwUNMEhGzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning test dataset as well\n",
        "\n",
        "df_test = pd.DataFrame(dataset['test'])\n",
        "df_test['clean_text'] = df_test['text'].apply(clean_text)\n",
        "df_test['clean_text'] = df_test['clean_text'].apply(remove_stopwords)\n",
        "df_test['clean_text'] = df_test['clean_text'].apply(lemmatize_text)\n",
        "\n",
        "#Print cleaned data\n",
        "display(df_test[['text', 'clean_text']].head())"
      ],
      "metadata": {
        "id": "M40wuYDMhH9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "_18OF5l0hPYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Review length distribution\n",
        "df_sample['text_length'] = df_sample['clean_text'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_sample['text_length'], bins=30, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of Review Length (in words)\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "#Show most common words\n",
        "all_words = \" \".join(df_sample['clean_text']).split()\n",
        "most_common = Counter(all_words).most_common(20)\n",
        "common_df = pd.DataFrame(most_common, columns=['Word', 'Frequency'])\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Frequency', y='Word', data=common_df, palette='viridis')\n",
        "plt.title(\"Top 20 Most Common Words in Reviews\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Words\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#Creating EDA to confirm distribution of labels\n",
        "df_sample['label'].value_counts().sort_index().plot(kind='bar', title='Label Distribution (Yelp)')"
      ],
      "metadata": {
        "id": "oOCz7JllhJqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}